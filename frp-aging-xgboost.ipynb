{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e6e13af",
   "metadata": {},
   "source": [
    "### XGBoost-based prediction of residual properties of aged FRP composites\n",
    "Copyright (C) 2025 João Paulo D. de S. Pereira\n",
    "License: GPL-3.0 (https://www.gnu.org/licenses/gpl-3.0.en.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51bad64-2c7a-4628-8a46-6b20c5ba53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from contextlib import suppress\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import r2_score, PredictionErrorDisplay\n",
    "from sklearn.model_selection import train_test_split, learning_curve, KFold  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, XGBRegressor\n",
    "import time\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17230632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record and display execution start time\n",
    "start_time = time.time()\n",
    "now = datetime.now()\n",
    "print(now.strftime(\"%d/%m/%Y %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84af78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "        target='Residual tensile modulus (GPa)',\n",
    "        file_name='training-dataset.csv',\n",
    "        min_target_counts=1,\n",
    "        cols_to_drop=['Author',\n",
    "            #'Type of resin',\n",
    "            'Isophtalic polyester resin',\n",
    "            'Orthophtalic polyester resin',\n",
    "            'Vinylester resin',\n",
    "            'Phenolic resin',\n",
    "            'Epoxy resin',\n",
    "            #'Type of fiber',\n",
    "            'Glass fiber',\n",
    "            'Carbon fiber',\n",
    "            #'Manufacturing process',\n",
    "            'Pultrusion',\n",
    "            'Hand lay-up',\n",
    "            'Filament winding',\n",
    "            'VARTM',\n",
    "            'Coupon descr.',\n",
    "            'Aging effect',\n",
    "            'Steady condition',\n",
    "            'Cyclic condition',\n",
    "            'Immersion',\n",
    "            'Moisture',\n",
    "            'Presence of salts',\n",
    "            #'pH',\n",
    "            #'Relative humidity',\n",
    "            'Sustained loading',\n",
    "            'Exposure time (hours)',\n",
    "            'Min. exposure temperature (ºC)',\n",
    "            'Max. exposure temperature (ºC)',\n",
    "            #'Temperature range (ºC)',\n",
    "            #'Unaged tensile modulus (GPa)',\n",
    "            #'Residual tensile modulus (GPa)'\n",
    "            'Tensile modulus retention'\n",
    "            ]\n",
    "        ):\n",
    "    \"\"\"Read and process a .CSV file into a filtered pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        target (string): Column name of the target property.\n",
    "        file_name (string): Name of training dataset file.\n",
    "        min_target_counts (int | float): Minimum count threshold for target\n",
    "            values. Rows with values appearing fewer times are dropped.\n",
    "        cols_to_drop (list): List with unimportant features to be removed from\n",
    "            the analysis.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Filtered dataset with only relevant columns. Rows\n",
    "            with values appearing fewer times are dropped.\n",
    "    \n",
    "    Notes:\n",
    "        The .CSV file path is hardcoded in this version and not\n",
    "            user-configurable. All files used herein shall be in the same folder\n",
    "        XGBoost is sparsity-aware. No need to drop missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Definition of dataset path\n",
    "    file_path=str(os.getcwd()) + '\\\\' + file_name\n",
    "    \n",
    "    # Read .CSV as pandas dataframe\n",
    "    df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "    # Drop columns that are not important for the analysis\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Remove unnamed columns\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Filters target property values that appear less than min_target_count times\n",
    "    value_counts = df[target].value_counts()\n",
    "    valid_values = value_counts[value_counts > min_target_counts - 1].index\n",
    "    df = df[df[target].isin(valid_values)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abs_error(y_test, y_pred):  \n",
    "    \"\"\"Calculate the maximum signed absolute error between predicted and actual\n",
    "        values.\n",
    "    \n",
    "    Args:\n",
    "        y_test (pandas.DataFrame): Actual values.\n",
    "        y_pred (numpy.ndarray): Predicted values from the model.\n",
    "    \n",
    "        Returns:\n",
    "            float: Maximum signed absolute error (y_pred - y_test), rounded to\n",
    "                3 decimals.\n",
    "        \n",
    "        Notes:\n",
    "            Positive values indicate overprediction, negative underprediction.\n",
    "            Both inputs must have the same length.    \n",
    "        \n",
    "        Example:\n",
    "        \\\\>>> y_test = pd.DataFrame([1.2, 3.4, 5.6])\n",
    "        \\\\>>> y_pred = np.array([1.0, 3.5, 5.2])\n",
    "        \\\\>>> get_abs_error(y_test, y_pred)\n",
    "        -0.4\n",
    "    \"\"\"\n",
    "    \n",
    "    y_test = y_test.tolist()\n",
    "    a = []\n",
    "    for i in range(len(y_test)):\n",
    "        a.append((y_pred[i] - y_test[i]))\n",
    "        \n",
    "    abs_error=0\n",
    "    if max(abs(i) for i in a) == max(a):\n",
    "        abs_error = max(a)\n",
    "    else:\n",
    "        abs_error = min(a)\n",
    "\n",
    "    return round(abs_error, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b960d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rel_error(y_test, y_pred):\n",
    "    \"\"\"Calculate the maximum signed relative error between predicted and actual\n",
    "        values.\n",
    "    \n",
    "    Args:\n",
    "        y_test (pandas.DataFrame): Actual values.\n",
    "        y_pred (numpy.ndarray): Predicted values from the model.\n",
    "    \n",
    "        Returns:\n",
    "            float: Maximum signed relative error ((y_pred - y_test) / y_test),\n",
    "                rounded to 3 decimals.\n",
    "        \n",
    "        Notes:\n",
    "            Positive values indicate overprediction, negative underprediction.\n",
    "            Both inputs must have the same length.\n",
    "        \n",
    "        Example:\n",
    "        \\\\>>> y_test = pd.DataFrame([1.2, 3.4, 5.6])\n",
    "        \\\\>>> y_pred = np.array([1.0, 3.5, 5.2])\n",
    "        \\\\>>> get_rel_error(y_test, y_pred)\n",
    "        -0.167\n",
    "    \"\"\"\n",
    "    \n",
    "    y_test = y_test.tolist()\n",
    "    a = []\n",
    "    for i in range(len(y_test)):\n",
    "        a.append((y_pred[i] - y_test[i])/y_test[i])\n",
    "        \n",
    "    rel_error=0\n",
    "    if max(abs(i) for i in a) == max(a):\n",
    "        rel_error = max(a)\n",
    "    else:\n",
    "        rel_error = min(a)\n",
    "\n",
    "    return round(rel_error, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(y_test, y_pred):\n",
    "    \"\"\"Calculate the mean absolute error (MAE) between predicted and actual values.\n",
    "    \n",
    "    Args:\n",
    "        y_test (pandas.DataFrame): Actual values.\n",
    "        y_pred (numpy.ndarray): Predicted values from the model.\n",
    "    \n",
    "        Returns:\n",
    "            float: MAE (Σ(|y_pred - y_test|) / len(y_test)), rounded to 3 decimals.\n",
    "        \n",
    "        Note:\n",
    "            Both inputs must have the same length.\n",
    "        \n",
    "        Example:\n",
    "        \\\\>>> y_test = pd.DataFrame([1.2, 3.4, 5.6])\n",
    "        \\\\>>> y_pred = np.array([1.0, 3.5, 5.2])\n",
    "        \\\\>>> get_mae(y_test, y_pred)\n",
    "        0.233\n",
    "    \"\"\"\n",
    "    \n",
    "    y_test = y_test.tolist()\n",
    "    a = 0\n",
    "    for i in range(len(y_test)):\n",
    "        a = a + abs(y_pred[i] - y_test[i])\n",
    "        \n",
    "    mae = a / len(y_test)\n",
    "\n",
    "    return round(mae, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(y_test, y_pred):\n",
    "    \"\"\"Calculate the root mean squared error (RMSE) between predicted and\n",
    "        actual values.\n",
    "    \n",
    "    Args:\n",
    "        y_test (pandas.DataFrame): Actual values.\n",
    "        y_pred (numpy.ndarray): Predicted values from the model.\n",
    "    \n",
    "        Returns:\n",
    "            float: RMSE ((Σ(|y_pred - y_test|)² / len(y_test))^0.5), rounded\n",
    "                to 3 decimals.\n",
    "        \n",
    "        Note:\n",
    "            Both inputs must have the same length.\n",
    "        \n",
    "        Example:\n",
    "        \\\\>>> y_test = pd.DataFrame([1.2, 3.4, 5.6])\n",
    "        \\\\>>> y_pred = np.array([1.0, 3.5, 5.2])\n",
    "        \\\\>>> get_rmse(y_test, y_pred)\n",
    "        0.265\n",
    "    \"\"\"\n",
    "    \n",
    "    y_test = y_test.tolist()\n",
    "    a = 0\n",
    "    for i in range(len(y_test)):\n",
    "        a = a + (y_pred[i] - y_test[i]) ** 2\n",
    "        \n",
    "    rmse = np.sqrt(a / len(y_test))\n",
    "\n",
    "    return round(rmse, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e326f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2(y_test, y_pred):\n",
    "    \"\"\"Calculate the coefficient of determination (or R² score) of predicted\n",
    "        values.\n",
    "    \n",
    "    Args:\n",
    "        y_test (pandas.DataFrame): Actual values.\n",
    "        y_pred (numpy.ndarray): Predicted values from the model.\n",
    "    \n",
    "        Returns:\n",
    "            float: R² score (1 - RSS/TSS), rounded to 3 decimals.\n",
    "        \n",
    "        Note:\n",
    "            RSS: Residual sum of squares (Σ(y_pred - y_test)²).\n",
    "            TSS: Total sum of squares (Σ(mean - y_test)²), i.e., variability of\n",
    "                the actual data.\n",
    "            An R² score equal to 1 indicates a perfect fit between predicted and\n",
    "                actual values. An R² equal to 0 means the prediction is no better\n",
    "                than the mean. A negative R² indicates the prediction is worse\n",
    "                than the mean.\n",
    "            Both inputs must have the same length.\n",
    "        \n",
    "        Example:\n",
    "        \\\\>>> y_test = pd.DataFrame([1.2, 3.4, 5.6])\n",
    "        \\\\>>> y_pred = np.array([1.0, 3.5, 5.2])\n",
    "        \\\\>>> get_r2(y_test, y_pred)\n",
    "        0.978\n",
    "    \"\"\"\n",
    "    \n",
    "    y_test = y_test.tolist()\n",
    "    mean = sum(y_test) / len(y_test)\n",
    "    rss = 0  # Residual sum of squares\n",
    "    tss = 0  # Total sum of squares\n",
    "    for i in range(len(y_test)):\n",
    "        rss = rss + (y_pred[i] - y_test[i]) ** 2\n",
    "        tss = tss + (mean - y_test[i]) ** 2\n",
    "\n",
    "    r2 = 1 - rss / tss\n",
    "\n",
    "    return round(r2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    \"\"\"Train XGBoost model with Bayesian optimization and predict the long-term\n",
    "        property retention of aged FRP composites.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_params: dict = {\n",
    "            'encoder': TargetEncoder(),\n",
    "            'clf': {\n",
    "                'random_state'  :0,\n",
    "                'booster'       :'gbtree',\n",
    "                'base_score'    :0.9,\n",
    "                'eval_metric'   :r2_score\n",
    "                },\n",
    "            'bayes_cv': {\n",
    "                'search_space': {\n",
    "                    'clf__max_depth'            :Integer(7, 10),\n",
    "                    'clf__n_estimators'         :Integer(500, 1000),\n",
    "                    'clf__learning_rate'        :Real(0.05, 0.1,\n",
    "                        prior='log-uniform'),\n",
    "                    'clf__subsample'            :Real(0.8, 1.0),\n",
    "                    'clf__colsample_bytree'     :Real(0.8, 1.0),\n",
    "                    'clf__reg_alpha'            :Real(0.1, 0.5),\n",
    "                    'clf__reg_lambda'           :Real(0.1, 0.5),\n",
    "                    'clf__gamma'                :Real(0.0, 0.05),\n",
    "                    'clf__min_child_weight'     :Integer(1, 2)\n",
    "                    },\n",
    "                'cv'            :5,\n",
    "                'n_iter'        :60,\n",
    "                'scoring'       :'r2',\n",
    "                'n_jobs'        :6\n",
    "                }\n",
    "            },\n",
    "            target='Residual tensile modulus (GPa)'\n",
    "        ):\n",
    "        \"\"\"Args:\n",
    "            model_params (dict, optional): Dictionary containing all model parameters.\n",
    "                Structure:\n",
    "                - encoder: Categorical variable encoder.\n",
    "                - clf: Estimator parameters.\n",
    "                    - random_state (int): Seed for reproducibility.\n",
    "                    - booster (str): Booster type.\n",
    "                    - base_score (float): Initial prediction value.\n",
    "                    - eval_metric  (Union[str, List[str], Callable, NoneType]):\n",
    "                        Metric for training result monitoring.\n",
    "                - bayes_cv: Bayesian optimization parameters (hyperparameter tuning).\n",
    "                    - search_space: Hyperparameter ranges for optimization.\n",
    "                    - cv: Number of cross-validation folds.\n",
    "                    - n_iter: Number of hyperparameter combinations to test.\n",
    "                    - n_jobs: Number of CPU cores to be used in parallel.\n",
    "            target (str, optional): Target property name.\n",
    "\n",
    "        Attributes:\n",
    "            - model_params (dict): Stored model parameters.\n",
    "            - target (str): Stored target property.\n",
    "            - cv_strategy (KFold): Cross-validation configuration. Shuffling is\n",
    "                enabled to ensure representative samples in each split.\n",
    "            - dataset (dict): Will store dataset information.\n",
    "            - n_runs (int): Will store number of models to be trained.\n",
    "            - trained_models (list): Will store models and metrics dicts after training.\n",
    "            - overall_stats (dict): Will store statiscally treated errors and R² score.\n",
    "            - best_model (XGBRegressor): Will store top-performing model.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_params   = model_params\n",
    "        self.target         = target\n",
    "        self.cv_strategy    = KFold(n_splits=5, shuffle=True)#, random_state=0)\n",
    "        self.dataset        = {\n",
    "            'dataset'           :0,\n",
    "            'X'                 :0,\n",
    "            'y'                 :0\n",
    "        }\n",
    "        self.n_runs         = 0\n",
    "        self.trained_models = []\n",
    "        self.overall_stats  = {\n",
    "            'abs_error_mean'    :0,\n",
    "            'abs_error_std'     :0,\n",
    "            'rel_error_mean'    :0,\n",
    "            'rel_error_std'     :0,\n",
    "            'mae_mean'          :0,\n",
    "            'mae_std'           :0,\n",
    "            'rmse_mean'         :0,\n",
    "            'rmse_std'          :0,\n",
    "            'r2_mean'           :0,\n",
    "            'r2_std'            :0\n",
    "        }\n",
    "        self.best_model     = None\n",
    "        \n",
    "    def create_model(self, log_transf=[]):\n",
    "        \"\"\"Sets up a Bayesian-optimized XGBoost regression pipeline including:\n",
    "            - Optional log-transformation of specified features\n",
    "            - Data preprocessing (encoding)\n",
    "            - XGBoost regressor with Bayesian hyperparameter optimization\n",
    "        \n",
    "        Args:\n",
    "            log_transf (list, optional): List of feature names to apply log10\n",
    "                transformation. Useful for handling skewed distributions and\n",
    "                outliers. Defaults to empty list.\n",
    "                \n",
    "        Returns:\n",
    "            skopt.BayesSearchCV: Configured Bayesian optimization search object with:\n",
    "                - Preprocessing pipeline (encoder + XGBoost)\n",
    "                - Parameter search space\n",
    "                - Cross-validation strategy\n",
    "                - Evaluation metrics\n",
    "                \n",
    "        Notes:\n",
    "            - Pipeline includes automatic encoding of categorical variables\n",
    "            - Uses XGBoost's 'reg:squarederror' objective by default\n",
    "        \"\"\"\n",
    "        \n",
    "        # Load dataset\n",
    "        df = load_dataset(target='Residual tensile modulus (GPa)')\n",
    "\n",
    "        # Log-transform features with outlier values\n",
    "        for i in range(len(log_transf)):\n",
    "            df[log_transf[i]] = np.log10(df[log_transf[i]])\n",
    "        \n",
    "        # Separate independent variables from target\n",
    "        X = df.drop(columns=self.target)\n",
    "        y = df[self.target]\n",
    "        \n",
    "        # Store dataset and splits\n",
    "        self.dataset['dataset'] = df\n",
    "        self.dataset['X'] = X\n",
    "        self.dataset['y'] = y\n",
    "\n",
    "        # Configure training pipeline and hyperparameter tuning\n",
    "        estimators = [\n",
    "            ('encoder', self.model_params['encoder']),\n",
    "            ('clf', XGBRegressor(\n",
    "                #random_state            =self.model_params['clf']['random_state'],\n",
    "                booster                 =self.model_params['clf']['booster'],\n",
    "                base_score              =self.model_params['clf']['base_score'],\n",
    "                eval_metric             =self.model_params['clf']['eval_metric'],\n",
    "                objective               ='reg:squarederror'\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "                \n",
    "        opt = BayesSearchCV(\n",
    "            Pipeline(steps=estimators),\n",
    "            self.model_params['bayes_cv']['search_space'],\n",
    "            cv                  = self.cv_strategy,\n",
    "            n_iter              = self.model_params['bayes_cv']['n_iter'],\n",
    "            scoring             = self.model_params['bayes_cv']['scoring'],\n",
    "            error_score         = 'raise',\n",
    "            random_state        = 0,\n",
    "            return_train_score  = True,\n",
    "            n_jobs              = self.model_params['bayes_cv']['n_jobs'],\n",
    "            n_points            = 1,\n",
    "            verbose             = False\n",
    "            )\n",
    "        \n",
    "        return opt\n",
    "\n",
    "    def train(self, n_runs=10, q=5, test_size=0.2, best_metric='r2', log_transf=[]):\n",
    "        \"\"\"Performs training pipeline for n_runs models including:\n",
    "            - Dataset preparation with optional log transformations\n",
    "            - Multiple train-test splits with target-stratified sampling\n",
    "            - Bayesian-optimized XGBoost training for each split\n",
    "            - Performance evaluation (abs. error, rel. error, MAE, RMSE and R²)\n",
    "\n",
    "        Args:\n",
    "            n_runs (int, optional): Number of independent training runs with\n",
    "                different data splits. Defaults to 10.\n",
    "            q (int, optional): Number of quantiles for stratified sampling of\n",
    "                continuous target variable. Defaults to 5.\n",
    "            test_size (float, optional): Proportion of dataset to use for\n",
    "                validation. Defaults to 0.2.\n",
    "            best_metric (str, optional): Metric used to select best model.\n",
    "                Options: 'r2' (R-squared), 'mae' (Mean Absolute Error), 'rmse'\n",
    "                (Root Mean Squared Error). Defaults to 'r2'.\n",
    "            log_transf (list, optional): List of feature names to apply log10\n",
    "                transformation. Defaults to empty list.\n",
    "\n",
    "        Returns:\n",
    "            None: Updates object attributes with trained models and statistics.\n",
    "\n",
    "        Attributes Modified:\n",
    "            trained_models (list): Stores dictionaries containing:\n",
    "                - Trained model objects\n",
    "                - Training/test data\n",
    "                - Prediction results\n",
    "                - Evaluation metrics\n",
    "                - Feature importance\n",
    "            overall_stats (dict): Aggregated statistics across all runs:\n",
    "                - Absolute/relative errors\n",
    "                - Mean ± std of MAE, RMSE, R²\n",
    "            best_model (dict): Information for best-performing model based on\n",
    "                best_metric\n",
    "\n",
    "        Notes:\n",
    "            - Uses quantile-based stratification to maintain target distribution\n",
    "            - Implements Bayesian optimization for hyperparameter tuning\n",
    "            - Stores model information for post-analysis\n",
    "        \"\"\"\n",
    "\n",
    "        # Create model\n",
    "        model = self.create_model(log_transf=log_transf)\n",
    "\n",
    "        # Load dataset\n",
    "        df = self.dataset['dataset']\n",
    "        X = df.drop(columns=self.target)\n",
    "        y = df[self.target]\n",
    "        \n",
    "        # Restart attributes\n",
    "        self.n_runs = n_runs\n",
    "        self.trained_models = []\n",
    "\n",
    "        # Bin the continuous target into categories for stratification\n",
    "        bins = pd.qcut(y, q=q, duplicates='drop')\n",
    "\n",
    "        loop_start = time.time()\n",
    "        for i in range(n_runs):\n",
    "\n",
    "            # Split data in training and test groups\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                test_size=test_size, stratify=bins\n",
    "                )\n",
    "        \n",
    "            # Train model\n",
    "            print('Starting fit #', i+1, sep='')\n",
    "            fit_start = time.time()\n",
    "            print(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\\n\"))\n",
    "            model.fit(X_train, y_train)\n",
    "            fit_finish = time.time()\n",
    "            print('Fit #', i+1, ' is finished! (%s s)\\n'\n",
    "                % round((fit_finish - fit_start), 1), sep='')\n",
    "            if i == n_runs - 1:\n",
    "                print('Total training duration: %s s\\n'\n",
    "                    % round((fit_finish - loop_start), 1), sep='')\n",
    "            y_pred = model.predict(X_test)\n",
    "            model_info = {\n",
    "                'id': i,\n",
    "                'model': model.best_estimator_,\n",
    "                'X_train': X_train,\n",
    "                'X_test': X_test,\n",
    "                'y_train': y_train,\n",
    "                'y_pred': y_pred,\n",
    "                'y_test': y_test,\n",
    "                'abs_error': get_abs_error(y_test, y_pred),\n",
    "                'rel_error': get_rel_error(y_test, y_pred),\n",
    "                'mae': get_mae(y_test, y_pred),\n",
    "                'rmse': get_rmse(y_test, y_pred),\n",
    "                'r2': get_r2(y_test, y_pred),\n",
    "                'best_cv_score': model.best_score_,\n",
    "                'best_params': model.best_params_,\n",
    "                'importance_plot': model.best_estimator_.steps[1][1],\n",
    "                'importance_score': model.best_estimator_.steps[1][1].\n",
    "                    get_booster().get_score(importance_type='weight')\n",
    "                }\n",
    "            self.trained_models.append(model_info)\n",
    "        \n",
    "        # Extract metrics, calculate and store statistics\n",
    "        abs_error = [self.trained_models[i]['abs_error']\n",
    "            for i in range(len(self.trained_models))]\n",
    "        abs_error_mean = np.mean(abs_error)\n",
    "        abs_error_std = np.std(abs_error)\n",
    "        self.overall_stats['abs_error_mean'] = abs_error_mean\n",
    "        self.overall_stats['abs_error_std'] = abs_error_std\n",
    "        print(f\"Abs. error: {abs_error_mean:.3f} ± {abs_error_std:.3f}\")\n",
    "\n",
    "        rel_error = [self.trained_models[i]['rel_error']\n",
    "            for i in range(len(self.trained_models))]\n",
    "        rel_error_mean = np.mean(rel_error)\n",
    "        rel_error_std = np.std(rel_error)\n",
    "        self.overall_stats['rel_error_mean'] = rel_error_mean\n",
    "        self.overall_stats['rel_error_std'] = rel_error_std\n",
    "        print(f\"Rel. error: {rel_error_mean:.3f} ± {rel_error_std:.3f}\")\n",
    "\n",
    "        mae = [self.trained_models[i]['mae']\n",
    "            for i in range(len(self.trained_models))]\n",
    "        mae_mean = np.mean(mae)\n",
    "        mae_std = np.std(mae)\n",
    "        self.overall_stats['mae_mean'] = mae_mean\n",
    "        self.overall_stats['mae_std'] = mae_std\n",
    "        print(f\"MAE: {mae_mean:.3f} ± {mae_std:.3f}\")\n",
    "\n",
    "        rmse = [self.trained_models[i]['rmse']\n",
    "            for i in range(len(self.trained_models))]\n",
    "        rmse_mean = np.mean(rmse)\n",
    "        rmse_std = np.std(rmse)\n",
    "        self.overall_stats['rmse_mean'] = rmse_mean\n",
    "        self.overall_stats['rmse_std'] = rmse_std\n",
    "        print(f\"RMSE: {rmse_mean:.3f} ± {rmse_std:.3f}\")\n",
    "\n",
    "        r2 = [self.trained_models[i]['r2']\n",
    "            for i in range(len(self.trained_models))]\n",
    "        r2_mean = np.mean(r2)\n",
    "        r2_std = np.std(r2)\n",
    "        self.overall_stats['r2_mean'] = r2_mean\n",
    "        self.overall_stats['r2_std'] = r2_std\n",
    "        print(f\"R²: {r2_mean:.2f} ± {r2_std:.2f}\\n\")\n",
    "    \n",
    "        # Store the best model\n",
    "        best_model = max(self.trained_models, key=lambda x: x['r2'])\n",
    "        self.best_model = best_model\n",
    "    \n",
    "    def plot_learning_curves(self, q=5, test_size=0.2, metrics=[1,1,1]):\n",
    "        \"\"\"...\n",
    "\n",
    "        Args:\n",
    "            ...\n",
    "            metrics (list): List with metrics to be plotted: MAE, RMSE, and R²,\n",
    "                respectively. 1 is plot, 0 is not plot. Default is to plot only R²\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"Plots MAE, RMSE, and R² learning curves to assess underfitting/overfitting\n",
    "        \n",
    "        Args:\n",
    "            q (int, optional): Number of quantiles for stratification.\n",
    "                Defaults to 5.\n",
    "            test_size (float, optional): Proportion of dataset to use for\n",
    "                validation. Defaults to 0.2.\n",
    "            metrics (list, optional): Which metrics to plot [MAE, RMSE, R²]. \n",
    "                1=plot, 0=skip. Defaults to [1,1,1] (all metrics).\n",
    "\n",
    "        Returns:\n",
    "            None: Displays matplotlib figures with learning curves and prints\n",
    "                mean ± std across CV folds.\n",
    "                \n",
    "        Notes:\n",
    "            - Uses stratified sampling to maintain target distribution\n",
    "        \"\"\"\n",
    "        \n",
    "        # Separate independent variables from target\n",
    "        X = self.dataset['X']\n",
    "        y = self.dataset['y']\n",
    "\n",
    "        # Bin the continuous target into categories for stratification\n",
    "        bins = pd.qcut(y, q=q, duplicates='drop')\n",
    "\n",
    "        # Split into train and test data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=bins, test_size=test_size)\n",
    "\n",
    "        # Define marker styles for different models\n",
    "        markers = ['o', 's', '^', 'D', 'v', 'p', '*', 'h', 'X', 'd']  # Add more if needed\n",
    "        fill = ['full', 'none']  # Alternate filled/unfilled\n",
    "\n",
    "        if metrics[0] == 1:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            train_score_total = []\n",
    "            test_score_total = []\n",
    "\n",
    "            for i in range(len(self.trained_models)):\n",
    "                train_sizes, train_scores, test_scores, fit_times, score_times = learning_curve(  \n",
    "                    self.trained_models[i]['model'], X, y, train_sizes=np.linspace(0.1, 1.0, 8),\n",
    "                    cv=self.cv_strategy, scoring='r2', return_times=True\n",
    "                )\n",
    "\n",
    "                # Converting MAE ro positive\n",
    "                train_scores = -train_scores\n",
    "                test_scores = -test_scores\n",
    "\n",
    "                print(f'\\nModel {i+1}')\n",
    "\n",
    "                d = []\n",
    "                for j, (a, b, c) in enumerate(zip(train_sizes, train_scores, test_scores)):\n",
    "                    # print(f\"CV {j+1}: {a} samples, train MAE = {b.mean():.3f}, valid MAE = {c.mean():.3f} ({sum(fit_times[j]):.2f} s)\")\n",
    "                    d.append(sum(fit_times[j]))\n",
    "                    if j == len(train_sizes) - 1:\n",
    "                        train_score_total.append(b)\n",
    "                        test_score_total.append(c)\n",
    "                # print(f\"{sum(d):.0f} s in total\")\n",
    "\n",
    "                # Calculate training and validation means and stds\n",
    "                train_mean = np.mean(train_scores, axis=1)\n",
    "                train_std = np.std(train_scores, axis=1)\n",
    "                test_mean = np.mean(test_scores, axis=1)\n",
    "                test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "                # Define marker and fill styles\n",
    "                marker = markers[i // 2 % len(markers)]  # Change marker every 2 models\n",
    "                fillstyle = fill[i % 2]  # Alternate fill for each model\n",
    "\n",
    "                # Plot training curve\n",
    "                plt.plot(\n",
    "                    train_sizes,\n",
    "                    train_mean,\n",
    "                    marker=marker,\n",
    "                    fillstyle=fillstyle,\n",
    "                    color='blue',\n",
    "                    linestyle='-',\n",
    "                    alpha=0.4,\n",
    "                    label=f'Train' if i == 0 else ''\n",
    "                    )\n",
    "                plt.fill_between(\n",
    "                    train_sizes,\n",
    "                    train_mean - train_std,\n",
    "                    train_mean + train_std,\n",
    "                    alpha=0.05,\n",
    "                    color='blue'\n",
    "                    )\n",
    "\n",
    "                # Plot validation curve\n",
    "                plt.plot(\n",
    "                    train_sizes,\n",
    "                    test_mean,\n",
    "                    marker=marker,\n",
    "                    color='red',\n",
    "                    linestyle='-',\n",
    "                    alpha=0.4,\n",
    "                    label=f'Validation' if i == 0 else ''\n",
    "                    )\n",
    "                plt.fill_between(\n",
    "                    train_sizes,\n",
    "                    test_mean - test_std,\n",
    "                    test_mean + test_std,\n",
    "                    alpha=0.05,\n",
    "                    color='red'\n",
    "                    )\n",
    "                \n",
    "            print(f\"Train MAE: {np.mean(train_score_total):.3f} ± {np.std(train_score_total):.3f}\")\n",
    "            print(f\"Valid MAE: {np.mean(test_score_total):.3f} ± {np.std(test_score_total):.3f}\")\n",
    "\n",
    "            # Plot formatting\n",
    "            plt.xlabel(\"Training set size\")\n",
    "            plt.ylabel(\"MAE\")\n",
    "            plt.ylim(-1.1, -0.4)  # Adjust based on your scores\n",
    "            plt.title('Learning curves across models')\n",
    "            plt.grid(True)\n",
    "\n",
    "            # Create custom legend\n",
    "            legend_elements = []\n",
    "            legend_elements.extend([\n",
    "                Line2D([0], [0], color='blue', lw=2, label='Training'),\n",
    "                Line2D([0], [0], color='red', lw=2, label='Validation')\n",
    "            ])\n",
    "\n",
    "            for i in range(len(self.trained_models)):\n",
    "                marker_idx = i // 2 % len(markers)\n",
    "                fill_idx = i % 2\n",
    "                legend_elements.append(Line2D(\n",
    "                    [0], [0],\n",
    "                    marker=markers[marker_idx],\n",
    "                    fillstyle=fill[fill_idx],\n",
    "                    color='gray',\n",
    "                    label=f'Model {i+1}',\n",
    "                    linestyle='None',\n",
    "                    markersize=10\n",
    "                ))\n",
    "\n",
    "            plt.legend(handles=legend_elements, ncol=3, loc='upper right')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        if metrics[1] == 1:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            train_score_total = []\n",
    "            test_score_total = []\n",
    "\n",
    "            for i in range(len(self.trained_models)):\n",
    "                train_sizes, train_scores, test_scores, fit_times, score_times = learning_curve(  \n",
    "                    self.trained_models[i]['model'], X, y, train_sizes=np.linspace(0.1, 1.0, 8),\n",
    "                    cv=self.cv_strategy, scoring='neg_root_mean_squared_error', return_times=True\n",
    "                )\n",
    "\n",
    "                # Converting MAE ro positive\n",
    "                train_scores = -train_scores\n",
    "                test_scores = -test_scores\n",
    "\n",
    "                print(f'\\nModel {i+1}')\n",
    "\n",
    "                d = []\n",
    "                for j, (a, b, c) in enumerate(zip(train_sizes, train_scores, test_scores)):\n",
    "                    # print(f\"CV {j+1}: {a} samples, train RMSE = {b.mean():.3f}, valid RMSE = {c.mean():.3f} ({sum(fit_times[j]):.2f} s)\")\n",
    "                    d.append(sum(fit_times[j]))\n",
    "                    if j == len(train_sizes) - 1:\n",
    "                        train_score_total.append(b)\n",
    "                        test_score_total.append(c)\n",
    "                # print(f\"{sum(d):.0f} s in total\")\n",
    "\n",
    "                # Calculate training and validation means and stds\n",
    "                train_mean = np.mean(train_scores, axis=1)\n",
    "                train_std = np.std(train_scores, axis=1)\n",
    "                test_mean = np.mean(test_scores, axis=1)\n",
    "                test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "                # Define marker and fill styles\n",
    "                marker = markers[i // 2 % len(markers)]  # Change marker every 2 models\n",
    "                fillstyle = fill[i % 2]  # Alternate fill for each model\n",
    "\n",
    "                # Plot training curve\n",
    "                plt.plot(\n",
    "                    train_sizes,\n",
    "                    train_mean,\n",
    "                    marker=marker,\n",
    "                    fillstyle=fillstyle,\n",
    "                    color='blue',\n",
    "                    linestyle='-',\n",
    "                    alpha=0.4,\n",
    "                    label=f'Train' if i == 0 else ''\n",
    "                    )\n",
    "                plt.fill_between(\n",
    "                    train_sizes,\n",
    "                    train_mean - train_std,\n",
    "                    train_mean + train_std,\n",
    "                    alpha=0.05,\n",
    "                    color='blue'\n",
    "                    )\n",
    "\n",
    "                # Plot validation curve\n",
    "                plt.plot(\n",
    "                    train_sizes,\n",
    "                    test_mean,\n",
    "                    marker=marker,\n",
    "                    color='red',\n",
    "                    linestyle='-',\n",
    "                    alpha=0.4,\n",
    "                    label=f'Validation' if i == 0 else ''\n",
    "                    )\n",
    "                plt.fill_between(\n",
    "                    train_sizes,\n",
    "                    test_mean - test_std,\n",
    "                    test_mean + test_std,\n",
    "                    alpha=0.05,\n",
    "                    color='red'\n",
    "                    )\n",
    "            \n",
    "            print(f\"Train RMSE: {np.mean(train_score_total):.3f} ± {np.std(train_score_total):.3f}\")\n",
    "            print(f\"Valid RMSE: {np.mean(test_score_total):.3f} ± {np.std(test_score_total):.3f}\")\n",
    "\n",
    "            # Plot formatting\n",
    "            plt.xlabel(\"Training set size\")\n",
    "            plt.ylabel(\"RMSE\")\n",
    "            plt.ylim(0, 7)  # Adjust based on your scores\n",
    "            plt.title('Learning curves across models')\n",
    "            plt.grid(True)\n",
    "\n",
    "            # Create custom legend\n",
    "            legend_elements = []\n",
    "            legend_elements.extend([\n",
    "                Line2D([0], [0], color='blue', lw=2, label='Training'),\n",
    "                Line2D([0], [0], color='red', lw=2, label='Validation')\n",
    "            ])\n",
    "\n",
    "            for i in range(len(self.trained_models)):\n",
    "                marker_idx = i // 2 % len(markers)\n",
    "                fill_idx = i % 2\n",
    "                legend_elements.append(Line2D(\n",
    "                    [0], [0],\n",
    "                    marker=markers[marker_idx],\n",
    "                    fillstyle=fill[fill_idx],\n",
    "                    color='gray',\n",
    "                    label=f'Model {i+1}',\n",
    "                    linestyle='None',\n",
    "                    markersize=10\n",
    "                ))\n",
    "\n",
    "            plt.legend(handles=legend_elements, ncol=3, loc='upper right')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        if metrics[2] == 1:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            train_score_total = []\n",
    "            test_score_total = []\n",
    "        \n",
    "            for i in range(len(self.trained_models)):\n",
    "                train_sizes, train_scores, test_scores, fit_times, score_times = learning_curve(  \n",
    "                    self.trained_models[i]['model'], X, y, train_sizes=np.linspace(0.1, 1.0, 8),\n",
    "                    cv=self.cv_strategy, scoring='r2', return_times=True\n",
    "                )\n",
    "\n",
    "                print(f'\\nModel {i+1}')\n",
    "\n",
    "                d =[]\n",
    "                for j, (a, b, c) in enumerate(zip(train_sizes, train_scores, test_scores)):\n",
    "                    # print(f\"CV {j+1}: {a} samples, train R² = {b.mean():.2f}, valid R² = {c.mean():.2f} ({sum(fit_times[j]):.2f} s)\")\n",
    "                    d.append(sum(fit_times[j]))\n",
    "                    if j == len(train_sizes) - 1:\n",
    "                        train_score_total.append(b)\n",
    "                        test_score_total.append(c)\n",
    "                # print(f\"{sum(d):.0f} s in total\")\n",
    "\n",
    "                # Calculate training and validation means and stds\n",
    "                train_mean = np.mean(train_scores, axis=1)\n",
    "                train_std = np.std(train_scores, axis=1)\n",
    "                test_mean = np.mean(test_scores, axis=1)\n",
    "                test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "                # Define marker and fill styles\n",
    "                marker = markers[i // 2 % len(markers)]  # Change marker every 2 models\n",
    "                fillstyle = fill[i % 2]  # Alternate fill for each model\n",
    "\n",
    "                # Plot training curve\n",
    "                plt.plot(\n",
    "                    train_sizes,\n",
    "                    train_mean,\n",
    "                    marker=marker,\n",
    "                    fillstyle=fillstyle,\n",
    "                    color='blue',\n",
    "                    linestyle='-',\n",
    "                    alpha=0.4,\n",
    "                    label=f'Train' if i == 0 else ''\n",
    "                    )\n",
    "                plt.fill_between(\n",
    "                    train_sizes,\n",
    "                    train_mean - train_std,\n",
    "                    train_mean + train_std,\n",
    "                    alpha=0.05,\n",
    "                    color='blue'\n",
    "                    )\n",
    "\n",
    "                # Plot validation curve\n",
    "                plt.plot(\n",
    "                    train_sizes,\n",
    "                    test_mean,\n",
    "                    marker=marker,\n",
    "                    color='red',\n",
    "                    linestyle='-',\n",
    "                    alpha=0.4,\n",
    "                    label=f'Validation' if i == 0 else ''\n",
    "                    )\n",
    "                plt.fill_between(\n",
    "                    train_sizes,\n",
    "                    test_mean - test_std,\n",
    "                    test_mean + test_std,\n",
    "                    alpha=0.05,\n",
    "                    color='red'\n",
    "                    )\n",
    "                    \n",
    "            print(f\"Train R²: {np.mean(train_score_total):.2f} ± {np.std(train_score_total):.2f}\")\n",
    "            print(f\"Valid R²: {np.mean(test_score_total):.2f} ± {np.std(test_score_total):.2f}\")\n",
    "            \n",
    "            # Plot formatting\n",
    "            plt.xlabel(\"Training set size\")\n",
    "            plt.ylabel(\"R² score\")\n",
    "            plt.ylim(0.4, 1.1)  # Adjust based on your scores\n",
    "            plt.title('Learning curves across models')\n",
    "            plt.grid(True)\n",
    "\n",
    "            # Create custom legend\n",
    "            legend_elements = []\n",
    "            legend_elements.extend([\n",
    "                Line2D([0], [0], color='blue', lw=2, label='Training'),\n",
    "                Line2D([0], [0], color='red', lw=2, label='Validation')\n",
    "            ])\n",
    "\n",
    "            for i in range(len(self.trained_models)):\n",
    "                marker_idx = i // 2 % len(markers)\n",
    "                fill_idx = i % 2\n",
    "                legend_elements.append(Line2D(\n",
    "                    [0], [0],\n",
    "                    marker=markers[marker_idx],\n",
    "                    fillstyle=fill[fill_idx],\n",
    "                    color='gray',\n",
    "                    label=f'Model {i+1}',\n",
    "                    linestyle='None',\n",
    "                    markersize=10\n",
    "                ))\n",
    "\n",
    "            plt.legend(handles=legend_elements, ncol=3, loc='lower right')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def plot_feature_distribution(self, main_feature='Exposure time (days)'):\n",
    "        \"\"\"Generates four diagnostic plots:\n",
    "            1. Boxplot of target variable distribution\n",
    "            2. Pairplot of all feature distributions\n",
    "            3. Scatter plot of main feature vs target\n",
    "            4. Value counts of main feature\n",
    "        \n",
    "        Args:\n",
    "            main_feature (str, optional): Feature to analyze in detail.\n",
    "                Defaults to 'Exposure time (days)'.\n",
    "        \n",
    "        Returns:\n",
    "            None: Displays matplotlib figures with feature distribution plots.\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.dataset['dataset']\n",
    "        \n",
    "        print('\\nTarget boxplot distribution')\n",
    "        sns.boxplot(x=self.dataset['y'])  # Check target\n",
    "        plt.show()\n",
    "\n",
    "        print('\\nOverall feature distribution')\n",
    "        sns.pairplot(df)  # Check feature distributions\n",
    "        # sns.pairplot(df, diag_kind='kde')  # Unncomment for smoothed curves\n",
    "        plt.show()\n",
    "\n",
    "        #  Outlier identification for most important feature\n",
    "        print('\\nMost important feature vs. target')\n",
    "        sns.scatterplot(x=main_feature, y=self.target, data=df)\n",
    "        plt.show()\n",
    "\n",
    "        # Check individual feature distribution\n",
    "        print('\\nMost important feature distribution')\n",
    "        display(self.dataset['X'][main_feature].value_counts())\n",
    "\n",
    "    def plot_pred_error(self):\n",
    "        \"\"\"Generate prediction error plots for all trained models.\n",
    "\n",
    "        Returns:\n",
    "            None: Displays matplotlib figures with error plots.\n",
    "    \n",
    "        Notes:\n",
    "            - Points on diagonal line indicate perfect predictions\n",
    "            - Systematic deviations suggest model bias\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(len(self.trained_models)):\n",
    "            print(f'\\nModel {i+1}')\n",
    "            display = PredictionErrorDisplay.from_predictions(\n",
    "                self.trained_models[i]['y_test'],\n",
    "                self.trained_models[i]['y_pred'],\n",
    "                kind=\"actual_vs_predicted\")\n",
    "            display.plot()\n",
    "            plt.show()\n",
    "    \n",
    "    def plot_importance(self):\n",
    "        \"\"\"Visualize feature importance scores for all trained models.\n",
    "        \n",
    "        Returns:\n",
    "            None: Displays matplotlib figures with feature distribution plots.\n",
    "\n",
    "        Notes:\n",
    "            - Importance types available: ['weight', 'gain', 'cover', 'total_gain',\n",
    "                'total_cover']\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(len(self.trained_models)):\n",
    "            print(f'\\nModel {i+1}')\n",
    "            self.trained_models[i]['importance_score']\n",
    "            with plt.style.context(\"ggplot\"):\n",
    "                fig = plt.figure(figsize=(6,6))\n",
    "                ax = fig.add_subplot(111)\n",
    "                xgb.plotting.plot_importance(self.trained_models[i]['importance_plot'], ax=ax)\n",
    "            plt.show()\n",
    "\n",
    "    def plot_tree(self, n_model=0, n_tree=0):\n",
    "        \"\"\"Visualize individual decision tree from XGBoost model.\n",
    "    \n",
    "        Args:\n",
    "            n_model (int, optional): Model index to visualize. Defaults to 0.\n",
    "            n_tree (int, optional): Tree number to visualize. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            None: Displays matplotlib figure with decision tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        with plt.style.context(\"ggplot\"):\n",
    "            fig = plt.figure(figsize=(25,10))\n",
    "            ax = fig.add_subplot(111)\n",
    "            xgb.plotting.plot_tree(self.trained_models[n_model]['importance_plot'], ax=ax, num_trees=n_tree)\n",
    "\n",
    "    def residual_analysis(self, feature):\n",
    "        \"\"\"Generates four diagnostic plots for each trained model to assess prediction quality:\n",
    "            1. Residual distribution histogram with kernel density estimation (KDE)\n",
    "            2. Residuals vs. actual values scatter plot\n",
    "            3. Residuals vs. specified feature scatter plot\n",
    "            4. Q-Q plot for normality assessment\n",
    "\n",
    "        Args:\n",
    "            feature (str): Name of feature to analyze (must match column\n",
    "                name in dataset).\n",
    "        \n",
    "        Returns:\n",
    "            None: Displays matplotlib figures with residual analysis plots and\n",
    "                prints 95% prediction interval (±1.96σ) for each model.\n",
    "\n",
    "        Notes:\n",
    "            - Red dashed line in scatter plots indicates perfect prediction\n",
    "                (residual = 0).\n",
    "            - Q-Q plot should follow straight line for normally distributed\n",
    "                residuals.\n",
    "            - Heteroscedasticity (changing variance) appears as funnel shape in\n",
    "                residual plots.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(len(self.trained_models)):\n",
    "            y_test = self.trained_models[i]['y_test']\n",
    "            y_pred = self.trained_models[i]['y_pred']\n",
    "            X_test = self.trained_models[i]['X_test']\n",
    "\n",
    "            residuals = y_test - y_pred\n",
    "            std_dev = residuals.std()\n",
    "            \n",
    "            print(f\"95% prediction interval: ±{1.96*std_dev:.2f} GPa\")\n",
    "\n",
    "            # 1. Residual Distribution Plot\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            sns.histplot(residuals, kde=True, bins=30)\n",
    "            plt.title(\"Residual Distribution\")\n",
    "            plt.xlabel(\"Prediction Error (GPa)\")\n",
    "            plt.show()\n",
    "\n",
    "            # 2. Residual vs. Actual Plot\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.scatter(y_test, residuals, alpha=0.3)\n",
    "            plt.axhline(0, color='r', linestyle='--')\n",
    "            plt.title(\"Residuals vs Actual Values\")\n",
    "            plt.xlabel(\"Actual Residual Modulus (GPa)\")\n",
    "            plt.ylabel(\"Prediction Error (GPa)\")\n",
    "            plt.show()\n",
    "\n",
    "            # 3. Residual vs. Key Features\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.scatter(X_test['Exposure time (days)'], residuals, alpha=0.3)\n",
    "            plt.axhline(0, color='r', linestyle='--')\n",
    "            plt.title(\"Residuals vs Exposure Time\")\n",
    "            plt.xlabel(\"Exposure Time (days)\")\n",
    "            plt.ylabel(\"Prediction Error (GPa)\")\n",
    "            plt.show()\n",
    "\n",
    "            from scipy import stats\n",
    "            stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "            plt.title(\"Q-Q Plot\")\n",
    "            plt.show()\n",
    "    \n",
    "    def lin_correl(self):\n",
    "        \"\"\"Calculate and display linear correlations (Pearson correlation coefficients)\n",
    "        between numerical features and target.\n",
    "\n",
    "        Returns:\n",
    "            None: Prints linear correlation of features in relation to target.\n",
    "        \n",
    "        Notes:\n",
    "            - Only considers numerical features (automatically excludes categoricals)\n",
    "            - Pearson correlation measures linear relationships (-1 to +1)\n",
    "            - Values near 0 indicate no linear correlation\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.dataset['X']\n",
    "        y = self.dataset['y']\n",
    "\n",
    "        # Calculate linear correlations\n",
    "        corr = pd.DataFrame(X).select_dtypes(include=['number']).corrwith(pd.Series(y))\n",
    "        print(corr.sort_values(ascending=False))\n",
    "\n",
    "    def pdp(self, i=0, feature=['Exposure time (days)']):\n",
    "        \"\"\"Plot partial dependence plots (PDP) for specified features in relation\n",
    "        to target while the influence from other features is marginalized.\n",
    "        \n",
    "        Args:\n",
    "            i (int): Index of training model to be used. Defaults to 0.\n",
    "            feature (list): List of feature names to analyze.\n",
    "\n",
    "        Returns:\n",
    "            None: Displays matplotlib figures with PDP.\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.dataset['X']\n",
    "\n",
    "        PartialDependenceDisplay.from_estimator(self.trained_models[i]['model'], X, features=[feature])\n",
    "        plt.show()\n",
    "\n",
    "    def holdout_test(self, file_name):\n",
    "        \"\"\"Evaluate models on holdout data to assess the model's capability to\n",
    "            extrapolate features beyond training data ranges.\n",
    "    \n",
    "        Args:\n",
    "            file_name (str): Name of holdout dataset file.\n",
    "            \n",
    "        Todo:\n",
    "            - Implement prediction and evaluation on holdout set\n",
    "            - Add metrics comparison to training performance\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(len(self.trained_models)):\n",
    "            self.predict()\n",
    "    \n",
    "    def bootstrap(self):\n",
    "        \"\"\"Calculate bootstrap confidence intervals for model metrics. Estimates\n",
    "            uncertainty in model performance by resampling with replacement.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing lists of bootstrapped metrics:\n",
    "                - 'mae': Mean absolute error values\n",
    "                - 'rmse': Root mean squared error values \n",
    "                - 'r2': R² values\n",
    "                \n",
    "        Todo:\n",
    "            - Implement bootstrap resampling procedure\n",
    "            - Calculate confidence intervals\n",
    "        \"\"\"\n",
    "\n",
    "        metrics = {\n",
    "            'mae': [],\n",
    "            'rmse': [],\n",
    "            'r2': []\n",
    "        }\n",
    "\n",
    "        model = self.best_model\n",
    "\n",
    "    def predict(self, model):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36dc493",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(target='Residual tensile modulus (GPa)')\n",
    "predictor.train(n_runs=4, log_transf=['Exposure time (days)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7893044",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.plot_learning_curves(metrics=[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.plot_feature_distribution(main_feature='Exposure time (days)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.plot_pred_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229811b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.plot_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc69e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.plot_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "df = load_dataset(target='Residual tensile modulus (GPa)')\n",
    "X = df.drop(columns=predictor.target)\n",
    "y = df[predictor.target]\n",
    "\n",
    "# Outer CV: Evaluate performance\n",
    "for i in range(len(predictor.trained_models)):\n",
    "    outer_scores = cross_val_score(\n",
    "        predictor.trained_models[i]['model'], X, y, \n",
    "        cv=KFold(5, shuffle=True), \n",
    "        scoring='r2'\n",
    "    )\n",
    "    print(f\"Unbiased R²: {np.mean(outer_scores):.3f} ± {np.std(outer_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.lin_correl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c772c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.residual_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad4766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fim do processamento\n",
    "now = datetime.now()\n",
    "\n",
    "print(now.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print(\"\\nProcessing time: %s seconds\\n\" % (round((time.time() - start_time), 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
